{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SUNNY SHABAN ALI - 22K4149 - 6B - ASSIGNMENT 1\n",
        "\n",
        "##### GitHub Repository for the project: https://www.github.com/sunnyallana/boolean-retrieval-model-pipeline\n",
        "\n",
        "\n",
        "## References:\n",
        "- 1. NLTK PorterStemmer: https://www.nltk.org/api/nltk.stem.porter.html\n",
        "- 2. Python Regular Expressions: https://docs.python.org/3/library/re.html\n",
        "- 3. Pickle Module: https://docs.python.org/3/library/pickle.html\n",
        "\n",
        "<br/>\n",
        "\n",
        "##### <strong>Kindly ensure that you have the Abstracts directory and Stopword-List.txt in the same folder as this code.<strong/>\n",
        "\n"
      ],
      "metadata": {
        "id": "AIxOI7RSdz-Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AN9hsezYx8T",
        "outputId": "2388c06f-6c00-4894-a876-50321a8174ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your query (or 'exit' to quit): neural networks /2\n",
            "Result-Set: 13, 21, 23, 24, 25, 26, 27, 28, 29, 30, 84, 88, 100, 114, 135, 145, 157, 162, 163, 164, 165, 174, 175, 176, 187, 194, 208, 210, 222, 223, 226, 245, 246, 247, 248, 267, 272, 273, 279, 280, 281, 284, 288, 291, 295, 303, 305, 308, 320, 322, 329, 337, 339, 344, 345, 348, 356, 371, 372, 373, 374, 375, 376, 381, 382, 395, 396, 397, 398, 401, 402, 406, 414, 415, 416, 417, 418, 420, 421, 429, 431, 440, 447\n",
            "Enter your query (or 'exit' to quit): neural information /2\n",
            "Result-Set: 26\n",
            "Enter your query (or 'exit' to quit): pattern\n",
            "Result-Set: 9, 10, 18, 21, 23, 26, 30, 34, 40, 50, 73, 118, 126, 127, 139, 145, 148, 155, 180, 186, 189, 194, 201, 209, 214, 216, 230, 231, 234, 238, 279, 280, 288, 326, 343, 350, 351, 368, 369, 383, 394, 406, 412, 413, 424, 425, 429, 446, 447\n",
            "Enter your query (or 'exit' to quit): exit\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import defaultdict\n",
        "import pickle\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess(text, stopwords, stemmer):\n",
        "    tokens = re.findall(r'\\w+', text.lower())\n",
        "    tokens = [token for token in tokens if token not in stopwords]\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "    return tokens\n",
        "\n",
        "# Read stopwords with Latin-1 encoding\n",
        "def read_stopwords(stopword_file):\n",
        "    with open(stopword_file, 'r', encoding='latin-1') as f:\n",
        "        stopwords = set(line.strip() for line in f)\n",
        "    return stopwords\n",
        "\n",
        "# Define a serializable defaultdict factory for positional index\n",
        "def list_defaultdict():\n",
        "    return defaultdict(list)\n",
        "\n",
        "# Build indexes\n",
        "def build_indexes(abstracts_dir, stopwords):\n",
        "    stemmer = PorterStemmer()\n",
        "    inverted_index = defaultdict(set)\n",
        "    positional_index = defaultdict(list_defaultdict)\n",
        "    all_docs = set()\n",
        "\n",
        "    for filename in os.listdir(abstracts_dir):\n",
        "        if filename.endswith('.txt'):\n",
        "            doc_id = filename.split('.')[0]\n",
        "            all_docs.add(doc_id)\n",
        "            with open(os.path.join(abstracts_dir, filename), 'r', encoding='latin-1') as f:\n",
        "                text = f.read()\n",
        "                tokens = preprocess(text, stopwords, stemmer)\n",
        "                for pos, term in enumerate(tokens):\n",
        "                    inverted_index[term].add(doc_id)\n",
        "                    positional_index[term][doc_id].append(pos)\n",
        "\n",
        "    # Convert defaultdict to dict for pickling\n",
        "    inverted_index = dict(inverted_index)\n",
        "    positional_index = {term: dict(docs) for term, docs in positional_index.items()}\n",
        "    return inverted_index, positional_index, all_docs\n",
        "\n",
        "# Save indexes\n",
        "def save_indexes(inverted_index, positional_index, filename='indexes.pkl'):\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump((inverted_index, positional_index), f)\n",
        "\n",
        "# Load indexes and reconstruct defaultdicts\n",
        "def load_indexes(filename='indexes.pkl'):\n",
        "    with open(filename, 'rb') as f:\n",
        "        inverted_index, positional_index = pickle.load(f)\n",
        "\n",
        "    # Rebuild defaultdicts for query processing\n",
        "    inverted_index_dd = defaultdict(set, {k: set(v) for k, v in inverted_index.items()})\n",
        "    positional_index_dd = defaultdict(list_defaultdict)\n",
        "    for term, docs in positional_index.items():\n",
        "        positional_index_dd[term] = defaultdict(list, docs)\n",
        "    return inverted_index_dd, positional_index_dd\n",
        "\n",
        "# Process Boolean query\n",
        "def process_boolean_query(query, inverted_index, all_docs, stopwords):\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = query.split()\n",
        "    current_result = None\n",
        "    prev_op = None\n",
        "    i = 0\n",
        "\n",
        "    while i < len(tokens):\n",
        "        token = tokens[i]\n",
        "        if token == \"NOT\":\n",
        "            if i + 1 >= len(tokens):\n",
        "                return set()\n",
        "            term = preprocess(tokens[i+1], stopwords, stemmer)\n",
        "            if not term:\n",
        "                docs = set()\n",
        "            else:\n",
        "                term = term[0]\n",
        "                docs = inverted_index.get(term, set())\n",
        "            not_docs = all_docs - docs\n",
        "            if current_result is None:\n",
        "                current_result = not_docs\n",
        "            else:\n",
        "                if prev_op == \"AND\":\n",
        "                    current_result &= not_docs\n",
        "                elif prev_op == \"OR\":\n",
        "                    current_result |= not_docs\n",
        "                prev_op = None\n",
        "            i += 2\n",
        "        elif token in [\"AND\", \"OR\"]:\n",
        "            prev_op = token\n",
        "            i += 1\n",
        "        else:\n",
        "            term = preprocess(token, stopwords, stemmer)\n",
        "            if not term:\n",
        "                docs = set()\n",
        "            else:\n",
        "                term = term[0]\n",
        "                docs = inverted_index.get(term, set())\n",
        "            if current_result is None:\n",
        "                current_result = docs\n",
        "            else:\n",
        "                if prev_op == \"AND\":\n",
        "                    current_result &= docs\n",
        "                elif prev_op == \"OR\":\n",
        "                    current_result |= docs\n",
        "                prev_op = None\n",
        "            i += 1\n",
        "\n",
        "    return current_result if current_result is not None else set()\n",
        "\n",
        "\n",
        "# Process proximity query\n",
        "def process_proximity_query(query, inverted_index, positional_index, stopwords):\n",
        "    stemmer = PorterStemmer()\n",
        "    parts = query.split()\n",
        "    for i, part in enumerate(parts):\n",
        "        if '/' in part:\n",
        "            if i < 2:\n",
        "                return set()\n",
        "            term1_part = parts[i-2]\n",
        "            term2_part = parts[i-1]\n",
        "            k = int(part.split('/')[1])\n",
        "            term1 = preprocess(term1_part, stopwords, stemmer)\n",
        "            term2 = preprocess(term2_part, stopwords, stemmer)\n",
        "            if not term1 or not term2:\n",
        "                return set()\n",
        "            term1 = term1[0]\n",
        "            term2 = term2[0]\n",
        "            common_docs = inverted_index.get(term1, set()) & inverted_index.get(term2, set())\n",
        "            result = set()\n",
        "            for doc_id in common_docs:\n",
        "                pos1_list = positional_index.get(term1, {}).get(doc_id, [])\n",
        "                pos2_list = positional_index.get(term2, {}).get(doc_id, [])\n",
        "                for pos1 in pos1_list:\n",
        "                    for pos2 in pos2_list:\n",
        "                        if abs(pos1 - pos2) - 1 <= k:\n",
        "                            result.add(doc_id)\n",
        "                            break\n",
        "                    if doc_id in result:\n",
        "                        break\n",
        "            return result\n",
        "    return set()\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Configuration\n",
        "    abstracts_dir = 'Abstracts'\n",
        "    stopword_file = 'Stopword-List.txt'\n",
        "    index_file = 'indexes.pkl'\n",
        "\n",
        "    # Load or build indexes with error handling\n",
        "    if os.path.exists(index_file):\n",
        "        try:\n",
        "            inverted_index, positional_index = load_indexes(index_file)\n",
        "            all_docs = set()\n",
        "            for term in inverted_index:\n",
        "                all_docs.update(inverted_index[term])\n",
        "            # Check if indexes are empty (invalid)\n",
        "            if not all_docs:\n",
        "                raise ValueError(\"Indexes are empty\")\n",
        "        except (EOFError, ValueError, pickle.UnpicklingError):\n",
        "            print(\"Corrupted or empty index file. Rebuilding indexes...\")\n",
        "            os.remove(index_file)  # Delete bad file\n",
        "            stopwords = read_stopwords(stopword_file)\n",
        "            inverted_index, positional_index, all_docs = build_indexes(abstracts_dir, stopwords)\n",
        "            save_indexes(inverted_index, positional_index, index_file)\n",
        "    else:\n",
        "        stopwords = read_stopwords(stopword_file)\n",
        "        inverted_index, positional_index, all_docs = build_indexes(abstracts_dir, stopwords)\n",
        "        if not all_docs:  # Check if no documents were processed\n",
        "            raise FileNotFoundError(f\"No .txt files found in {abstracts_dir} directory\")\n",
        "        save_indexes(inverted_index, positional_index, index_file)\n",
        "\n",
        "    # Command line interface\n",
        "    while True:\n",
        "        query = input(\"Enter your query (or 'exit' to quit): \").strip()\n",
        "        if query.lower() == 'exit':\n",
        "            break\n",
        "        if '/ ' in query:  # Handle proximity queries with space after /\n",
        "            query = query.replace('/ ', '/')\n",
        "        if '/' in query:\n",
        "            result = process_proximity_query(query, inverted_index, positional_index, read_stopwords(stopword_file))\n",
        "        else:\n",
        "            result = process_boolean_query(query, inverted_index, all_docs, read_stopwords(stopword_file))\n",
        "        print(\"Result-Set:\", ', '.join(sorted(result, key=int)) if result else \"No results found\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}